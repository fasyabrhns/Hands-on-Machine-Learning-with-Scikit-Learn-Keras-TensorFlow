# Hands-on-Machine-Learning-with-Scikit-Learn-Keras-TensorFlow


<img width="300" src="https://github.com/user-attachments/assets/ifbccc63-e7d2-42dd-ac60-6688f58f9691" alt="Book Cover"/>  


---

## ðŸŒŸ Introduction  
This repository contains my personal notes and code implementation from:  

**AurÃ©lien GÃ©ron â€“ Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (2nd Edition, Oâ€™Reilly).**  

The purpose of this repo is to:  
- Strengthen my understanding of Machine Learning & Deep Learning.  
- Document key theories in a concise way.  
- Reproduce all notebooks from each chapter.  

---

## ðŸ“‘ Part I â€“ The Fundamentals of Machine Learning  

Machine Learning enables systems to learn from data and make predictions without being explicitly programmed. Instead of designing rules, we let algorithms extract patterns from examples.  

### Chapters:  

1. **The Machine Learning Landscape**  
   - What ML is, categories (supervised, unsupervised, reinforcement).  
   - Real-world applications (spam filters, recommender systems, vision, NLP).  
   - Key challenges: overfitting, underfitting, insufficient data.  

2. **End-to-End Machine Learning Project**  
   - Full pipeline: data gathering â†’ exploration â†’ cleaning â†’ model training â†’ evaluation â†’ deployment.  
   - Using `Scikit-Learn` for housing price prediction project.  

3. **Classification**  
   - Binary vs multiclass classification.  
   - Metrics: accuracy, precision, recall, F1-score, confusion matrix.  
   - Tools: `Logistic Regression`, `k-NN`, etc.  

4. **Training Models**  
   - Linear Regression, Polynomial Regression.  
   - Gradient Descent variants (Batch, Stochastic, Mini-batch).  
   - Regularization: Ridge, Lasso, Elastic Net.  

5. **Support Vector Machines (SVMs)**  
   - Linear SVMs and margins.  
   - Kernel trick for nonlinear decision boundaries.  
   - Soft margin and hyperparameter tuning.  

6. **Decision Trees**  
   - Splitting criteria: Gini, Entropy.  
   - Visualization and interpretability.  
   - Avoiding overfitting with pruning.  

7. **Ensemble Learning & Random Forests**  
   - Bagging (Bootstrap Aggregating).  
   - Boosting (AdaBoost, Gradient Boosting).  
   - Random Forests as a strong ensemble baseline.  

8. **Dimensionality Reduction**  
   - Feature extraction vs feature selection.  
   - PCA, Kernel PCA, and Manifold Learning (t-SNE).  
   - Trade-offs in reducing dimensionality.  

---
